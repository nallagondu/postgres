{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGzRP3ryJu0+O1GaQP3V5J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nallagondu/postgres/blob/main/postgres_training_5_Monitoring_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vlsz3AoQWQM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PostgreSQL Monitoring Guide: Comprehensive Overview**\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Overview\n",
        "\n",
        "PostgreSQL monitoring ensures that the database runs smoothly, identifies performance bottlenecks, and maintains availability. In this session, three levels of monitoring were discussed:\n",
        "\n",
        "1. **Grafana + Prometheus Dashboard (Proactive/Reactive Monitoring)**\n",
        "2. **Manual Monitoring via CLI Scripts & Commands**\n",
        "3. **PGAdmin GUI (To be covered in next session)**\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Grafana + Prometheus Architecture\n",
        "\n",
        "**Components**:\n",
        "\n",
        "* **Node Exporter**: Agent on DB servers collecting CPU, memory, disk, and process data\n",
        "* **Prometheus Server**: Pulls metrics from Node Exporter and stores time-series data\n",
        "* **Grafana**: Dashboards that visualize Prometheus data\n",
        "\n",
        "**Metrics Collected**:\n",
        "\n",
        "| Metric Category      | Examples                            |\n",
        "| -------------------- | ----------------------------------- |\n",
        "| CPU Usage            | %CPU, Load Average, Per Core Stats  |\n",
        "| Memory Usage         | Total, Used, Free, Cache            |\n",
        "| Disk Usage           | File system size, Used space        |\n",
        "| PostgreSQL Health    | DB Uptime, Connection Count, Role   |\n",
        "| Replication          | Lag, Status, Role (Primary/Standby) |\n",
        "| Blocking & Deadlocks | Blocking Queries, Deadlock Events   |\n",
        "| Table Bloat          | %Bloat, Table size, Index info      |\n",
        "| Query Performance    | Top Queries by Time, Frequency      |\n",
        "\n",
        "**Retention**: Default 7-day metrics retention\n",
        "\n",
        "**Refresh Rate**: 30 seconds (configurable)\n",
        "\n",
        "---\n",
        "\n",
        "### üî¢ Key Commands (For Manual Monitoring)\n",
        "\n",
        "| Purpose                  | Command or Tool                                     |\n",
        "| ------------------------ | --------------------------------------------------- |\n",
        "| Check PostgreSQL status  | `systemctl status postgresql`                       |\n",
        "| Check replication status | `SELECT * FROM pg_stat_replication;`                |\n",
        "| View locks and blocks    | `SELECT * FROM pg_locks;`                           |\n",
        "| Table bloat estimate     | Check pgstattuple or `pg_bloat_check.sql`           |\n",
        "| Active queries           | `SELECT * FROM pg_stat_activity;`                   |\n",
        "| Database size            | `SELECT pg_size_pretty(pg_database_size('dbname'))` |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öñÔ∏è Alert Manager & Silence\n",
        "\n",
        "* **Alert Conditions**:\n",
        "\n",
        "  * CPU > 90% = Critical\n",
        "  * Disk Full = Critical\n",
        "  * Replication Lag > Threshold\n",
        "  * Node Exporter Down = Alert\n",
        "* **Action**:\n",
        "\n",
        "  * Alert sent to L1 team (via ServiceNow)\n",
        "  * Escalated to DBAs if unresolved\n",
        "* **Silence (Blackout)**:\n",
        "\n",
        "  * Used during maintenance\n",
        "  * Prevents false alarms\n",
        "  * Can be configured for 1 hour to N days\n",
        "\n",
        "---\n",
        "\n",
        "### üîß Examples of Issues Caught\n",
        "\n",
        "| Issue Type      | Tool/Section            | Resolution Action                  |\n",
        "| --------------- | ----------------------- | ---------------------------------- |\n",
        "| High CPU Usage  | Grafana CPU Panel       | Investigate queries, check load    |\n",
        "| Table Bloat     | Bloat Panel             | Full VACUUM (by DBA)               |\n",
        "| Query Slowness  | Top Queries by Duration | Optimize query/indexing            |\n",
        "| Replication Lag | Replication Metrics     | Restart streaming, check WAL files |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùì Interview Questions & Answers\n",
        "\n",
        "**Q1:** What are the tools used for PostgreSQL monitoring?\n",
        "**A1:** Grafana, Prometheus, PGAdmin, and manual CLI scripts.\n",
        "\n",
        "**Q2:** What is the purpose of Node Exporter?\n",
        "**A2:** It collects OS and DB metrics from PostgreSQL hosts for Prometheus.\n",
        "\n",
        "**Q3:** How do you detect table bloat?\n",
        "**A3:** Use Grafana bloat panel or queries like pgstattuple to check fragmentation.\n",
        "\n",
        "**Q4:** What is a 'Silence' in Alertmanager?\n",
        "**A4:** It is a mechanism to mute alerts during maintenance or planned downtimes.\n",
        "\n",
        "**Q5:** What is replication lag and where is it shown?\n",
        "**A5:** The delay between primary and standby DB. Shown in Grafana under Replication Metrics.\n",
        "\n",
        "---\n",
        "\n",
        "### üåê Reference & Notes\n",
        "\n",
        "* Tools: Grafana, Prometheus, AlertManager\n",
        "* PostgreSQL Version: Custom port used (e.g., 9432)\n",
        "* Only production servers monitored with this setup\n",
        "* Alerts handled initially by L1 team; escalated as needed\n",
        "\n",
        "---\n",
        "\n",
        "Would you like this converted into:\n",
        "\n",
        "* PDF\n",
        "* Excel Sheet\n",
        "* PNG Visual Dashboard\n",
        "* Grafana Dashboard JSON Export\n",
        "\n",
        "Let me know if you want PGAdmin examples, PromQL queries, or sample Grafana panels next.\n"
      ],
      "metadata": {
        "id": "sXBM5C8lTPUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî≠ Prometheus & AlertManager in KOB Architecture\n",
        "üß± 1. Prometheus in KOB\n",
        "Prometheus is the core metrics collection and monitoring engine used to monitor PostgreSQL clusters and infrastructure in the KOB platform.\n",
        "\n",
        "üîπ Prometheus Components in KOB\n",
        "Component\tRole\n",
        "prometheus-server\tScrapes metrics from exporters and stores time-series data\n",
        "node-exporter\tCollects OS-level metrics (CPU, memory, disk)\n",
        "postgres-exporter\tExposes PostgreSQL metrics (connections, WALs, replication, locks)\n",
        "thanos-sidecar\tExtends Prometheus by pushing data to object storage (S3)\n",
        "kube-state-metrics\tCollects Kubernetes resource metrics (Pods, Deployments, PVC usage)\n",
        "\n",
        "‚öôÔ∏è Prometheus Workflow in KOB\n",
        "mermaid\n",
        "Copy\n",
        "Edit\n",
        "graph TD\n",
        "  A[PostgreSQL DB Nodes] -->|Metrics| B[prometheus-postgres-exporter]\n",
        "  B --> C[Prometheus Server]\n",
        "  C --> D[Time-Series DB]\n",
        "  C --> E[AlertManager]\n",
        "  C --> F[Grafana Dashboard]\n",
        "  C --> G[Thanos Sidecar]\n",
        "  G --> H[S3 Object Storage]\n",
        "  C --> I[Kube-State-Metrics]\n",
        "üö® 2. AlertManager in KOB\n",
        "AlertManager handles alerting rules, grouping, notification routing, and silencing.\n",
        "\n",
        "üîπ AlertManager Components\n",
        "Component\tDescription\n",
        "alertmanager\tReceives alerts from Prometheus and sends notifications\n",
        "config.yaml\tDefines routes, receivers, inhibition rules, and templates\n",
        "silence rules\tTemporarily mute alerts (used during maintenance or blackout)\n",
        "receivers\tEmail, Slack, ServiceNow, PagerDuty, Webhook\n",
        "\n",
        "üì£ AlertManager Flow in KOB\n",
        "Prometheus evaluates alert rules (e.g., CPU > 90%, replication lag)\n",
        "\n",
        "If a condition is breached, Prometheus pushes alert to AlertManager\n",
        "\n",
        "AlertManager routes it to a receiver (e.g., ServiceNow, Slack)\n",
        "\n",
        "If in Silence, the alert is muted and not forwarded\n",
        "\n",
        "L1 teams act, escalate to DBAs if unresolved\n",
        "\n",
        "üßæ Sample Alert Rules\n",
        "yaml\n",
        "Copy\n",
        "Edit\n",
        "groups:\n",
        "- name: postgres.rules\n",
        "  rules:\n",
        "  - alert: PostgreSQLHighCPU\n",
        "    expr: avg by (instance) (rate(process_cpu_seconds_total{job=\"postgres\"}[5m])) > 0.9\n",
        "    for: 2m\n",
        "    labels:\n",
        "      severity: critical\n",
        "    annotations:\n",
        "      summary: \"High CPU usage on {{ $labels.instance }}\"\n",
        "      description: \"CPU usage is above 90% for more than 2 minutes.\"\n",
        "üîê Silence (Blackout) Use in KOB\n",
        "Use Case\tExample\n",
        "DB Patch Upgrade\tSilence created for affected DB node\n",
        "Storage Maintenance\tMute alerts on PVC/Disk\n",
        "Weekly Restart\tSuppress false ‚ÄúInstanceDown‚Äù alerts\n",
        "\n",
        "üß∞ KOB Prometheus Helm Chart Includes:\n",
        "prometheus-server.yaml\n",
        "\n",
        "alertmanager.yaml\n",
        "\n",
        "rules.yaml\n",
        "\n",
        "thanos-sidecar.yaml\n",
        "\n",
        "grafana-dashboards/\n",
        "\n",
        "serviceMonitor and PodMonitor for Kubernetes scraping\n",
        "\n",
        "üì¶ Storage and Scaling with Thanos (Optional in KOB)\n",
        "Component\tRole\n",
        "Thanos Sidecar\tPushes metrics to S3 / long-term object store\n",
        "Thanos Store\tRetrieves historical data from S3\n",
        "Thanos Querier\tMerges data from multiple Prometheus + Thanos stores\n",
        "Thanos Compactor\tDeduplicates and compacts old data"
      ],
      "metadata": {
        "id": "2VsiGJW9UcQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üö® AlertManager in KOB: Architecture + Silence Logic\n",
        "üß† Purpose\n",
        "AlertManager is used to route and manage alerts triggered by Prometheus based on defined thresholds. It helps ensure issues are escalated properly without spamming teams during maintenance windows.\n",
        "\n",
        "üîÅ KOB Workflow for Alert Handling\n",
        "mermaid\n",
        "Copy\n",
        "Edit\n",
        "graph TD\n",
        "  A[Prometheus Rules] -->|Alert Fired| B[AlertManager]\n",
        "  B --> C{Check Silence?}\n",
        "  C -- Yes --> D[Suppress Alert (No Action)]\n",
        "  C -- No --> E[Route Alert]\n",
        "  E --> F[L1 Team (ServiceNow)]\n",
        "  F --> G{Resolved?}\n",
        "  G -- Yes --> H[Incident Closed]\n",
        "  G -- No --> I[Escalate to DBAs]\n",
        "üßæ Key Concepts from Session\n",
        "Concept\tExplanation\n",
        "Silence\tTemporary mute (blackout) applied to a DB/server instance during maintenance\n",
        "Retention\tSilences can be configured for hours, days, or even recurring\n",
        "Reuse\tExpired silences can be recreated quickly ‚Äì no need to start from scratch\n",
        "Use Case\tApplied before shutdown, migration, patching to avoid false alerts\n",
        "L1 Routing\tIf not silenced, AlertManager routes alerts (CPU, DB down, WAL lag) to L1\n",
        "Escalation\tIf unresolved, L1 escalates incidents to DBAs or Engineering\n",
        "\n",
        "üõ°Ô∏è Silence/Blackout Strategy in KOB\n",
        "Scenario\tSilence Needed?\tDuration\tReason\n",
        "DB Patch Upgrade\t‚úÖ Yes\t2‚Äì3 hrs\tAvoid false CPU/Disk alerts during restarts\n",
        "Replication Maintenance\t‚úÖ Yes\t1‚Äì2 hrs\tPrevent lag-related alerts\n",
        "Table Vacuum/Reindex\t‚úÖ Yes\t30 min‚Äì1h\tDisk/CPU usage may spike\n",
        "PostgreSQL Restart\t‚úÖ Yes\t30 mins\tAvoid false ‚ÄúDB Down‚Äù or ‚ÄúExporter Down‚Äù alerts\n",
        "\n",
        "üîß Silence Example (as discussed)\n",
        "You can create silence by selecting:\n",
        "\n",
        "Instance name: mydb01:9100\n",
        "\n",
        "Duration: 1 hour, 2 hours, 7 days, etc.\n",
        "\n",
        "Comment: e.g., \"Planned upgrade by DBA team\"\n",
        "\n",
        "Silence can be edited, expired, or recreated after expiry\n",
        "\n",
        "This prevents unnecessary L1 incident creation (especially across systems like MongoDB, SQL, Oracle, Redis)\n",
        "\n"
      ],
      "metadata": {
        "id": "FEuRxaXTUdlQ"
      }
    }
  ]
}